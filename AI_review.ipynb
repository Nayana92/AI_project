{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def getcontent(path):\n",
    "# path = 'txt_sentoken/pos/*.txt'   \n",
    "    files = glob.glob(path)   \n",
    "    listofreviews=[]\n",
    "    for name in files: # 'file' is a builtin type, 'name' is a less-ambiguous variable name\n",
    "        with open(name) as f: # No need to specify 'r': this is the default.\n",
    "            #sys.stdout.write(f.read())\n",
    "            contents=f.read()\n",
    "            contents=contents.replace(',','')\n",
    "            contents=contents.replace('\\n','')\n",
    "            listofreviews.append([''.join(contents)])\n",
    "    return listofreviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "posreviews = getcontent('txt_sentoken/pos/*.txt')\n",
    "negreviews = getcontent('txt_sentoken/neg/*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "posdf = pd.DataFrame({'review':posreviews})\n",
    "posdf['label'] = 1\n",
    "negdf = pd.DataFrame({'review':negreviews})\n",
    "negdf['label'] = -1 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = pd.concat([posdf,negdf],ignore_index=True)\n",
    "alldata['review'] = alldata['review'].apply(', '.join)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def getwordlist(text):\n",
    "    # Convert words to lower case and split them\n",
    "    text = ''.join(text)\n",
    "    text = text.lower().split()    \n",
    "    text = \" \".join(text)    \n",
    "    #Remove Special Characters\n",
    "    text=re.sub(r'[^a-z\\d ]','',text)    \n",
    "    #Replace Numbers\n",
    "    text=re.sub(r'\\d+','n',text)\n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata['review'] = alldata.review.apply(getwordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>films adapted from comic books have had plenty...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>every now and then a movie comes along from a ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>youve got mail works alot better than it deser...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>jaws  is a rare film that grabs your attentio...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moviemaking is a lot like being the general ma...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  label\n",
       "0  films adapted from comic books have had plenty...      1\n",
       "1  every now and then a movie comes along from a ...      1\n",
       "2  youve got mail works alot better than it deser...      1\n",
       "3   jaws  is a rare film that grabs your attentio...      1\n",
       "4  moviemaking is a lot like being the general ma...      1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(alldata['review'], alldata['label'], test_size=0.20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train))\n",
    "print(len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7725\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.94      0.55      0.69       187\n",
      "    negative       0.71      0.97      0.82       213\n",
      "\n",
      "   micro avg       0.77      0.77      0.77       400\n",
      "   macro avg       0.82      0.76      0.76       400\n",
      "weighted avg       0.82      0.77      0.76       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sgd = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train, y_train)\n",
    "\n",
    "# %%time\n",
    "\n",
    "y_pred = sgd.predict(X_test)\n",
    "my_tags = ['positive','negative']\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.82      0.80      0.81       187\n",
      "    negative       0.83      0.85      0.84       213\n",
      "\n",
      "   micro avg       0.82      0.82      0.82       400\n",
      "   macro avg       0.82      0.82      0.82       400\n",
      "weighted avg       0.82      0.82      0.82       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC   \n",
    "svm = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SVC(kernel='linear')),\n",
    "               ])\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# %%time\n",
    "\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "my_tags = ['positive','negative']\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.62      0.86      0.72       187\n",
      "    negative       0.81      0.54      0.65       213\n",
      "\n",
      "   micro avg       0.69      0.69      0.69       400\n",
      "   macro avg       0.72      0.70      0.69       400\n",
      "weighted avg       0.72      0.69      0.68       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC   \n",
    "svm_rbf = Pipeline([\n",
    "                ('vect', CountVectorizer()),\n",
    "                ('clf', SVC(kernel='rbf')),\n",
    "               ])\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# %%time\n",
    "\n",
    "y_pred = svm_rbf.predict(X_test)\n",
    "\n",
    "my_tags = ['positive','negative']\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def word2vectoizer(x_data):\n",
    "    x_data_embeddings = []\n",
    "    wordVector = np.zeros(300)\n",
    "    count = 0\n",
    "    x_data = x_data.split(\" \")\n",
    "    for words in x_data:\n",
    "        count += 1\n",
    "        if words in w2v.vocab:\n",
    "            wordVector += w2v.word_vec(words)\n",
    "    if(count != 0):\n",
    "        wordVector /= count\n",
    "    x_data_embeddings.append(wordVector)\n",
    "    return wordVector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.7825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.79      0.81      0.80       213\n",
      "    negative       0.78      0.75      0.76       187\n",
      "\n",
      "   micro avg       0.78      0.78      0.78       400\n",
      "   macro avg       0.78      0.78      0.78       400\n",
      "weighted avg       0.78      0.78      0.78       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC   \n",
    "svm_embedding = Pipeline([\n",
    "#                 \"word2vec vectorizer\", MeanEmbeddingVectorizer(model)\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SVC(kernel='linear')),\n",
    "               ])\n",
    "\n",
    "test_tokenized = X_test.apply(word2vectoizer)\n",
    "test_tokenized = pd.DataFrame(test_tokenized.values.tolist())\n",
    "train_tokenized = X_train.apply(word2vectoizer)\n",
    "train_tokenized = pd.DataFrame(train_tokenized.values.tolist())\n",
    "svm_embedding.fit(train_tokenized, y_train)\n",
    "                              \n",
    "# %%time\n",
    "\n",
    "y_pred = svm_embedding.predict(test_tokenized)\n",
    "\n",
    "my_tags = ['positive','negative']\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.4925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.00      0.00      0.00       203\n",
      "    negative       0.49      1.00      0.66       197\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       400\n",
      "   macro avg       0.25      0.50      0.33       400\n",
      "weighted avg       0.24      0.49      0.33       400\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC   \n",
    "svm_embedding_rbf = Pipeline([\n",
    "#                 \"word2vec vectorizer\", MeanEmbeddingVectorizer(model)\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SVC(kernel='rbf')),\n",
    "               ])\n",
    "\n",
    "test_tokenized = X_test.apply(word2vectoizer)\n",
    "test_tokenized = pd.DataFrame(test_tokenized.values.tolist())\n",
    "train_tokenized = X_train.apply(word2vectoizer)\n",
    "train_tokenized = pd.DataFrame(train_tokenized.values.tolist())\n",
    "svm_embedding_rbf.fit(train_tokenized, y_train)\n",
    "                              \n",
    "# %%time\n",
    "\n",
    "y_pred = svm_embedding_rbf.predict(test_tokenized)\n",
    "\n",
    "my_tags = ['positive','negative']\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# import numpy as np\n",
    "\n",
    "# def w2v_tokenize_text(text):\n",
    "#     tokens = []\n",
    "# #     print(text)\n",
    "#     for sent in nltk.sent_tokenize(text):\n",
    "#         for word in nltk.word_tokenize(sent):\n",
    "#             if len(word) < 2:\n",
    "#                 continue\n",
    "#             tokens.append(word)\n",
    "#     return tokens\n",
    "    \n",
    "# # train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "# test_tokenized = X_test.apply(w2v_tokenize_text)#X_test.apply(lambda r: w2v_tokenize_text(r['comments'])).values\n",
    "# train_tokenized = X_train.apply(w2v_tokenize_text)#X_train.apply(lambda r: w2v_tokenize_text(r['comments'])).values\n",
    "\n",
    "# X_train_word_average = word_averaging_list(model,train_tokenized)\n",
    "# X_test_word_average = word_averaging_list(model,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def word_averaging(wv, words):\n",
    "#     all_words, mean = set(), []\n",
    "    \n",
    "#     for word in words:\n",
    "#         if isinstance(word, np.ndarray):\n",
    "#             mean.append(word)\n",
    "#         elif word in wv.vocab:\n",
    "#             mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
    "#             all_words.add(wv.vocab[word].index)\n",
    "\n",
    "#     if not mean:\n",
    "# #         logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "#         # FIXME: remove these examples in pre-processing\n",
    "#         return np.zeros(wv.vector_size,)\n",
    "\n",
    "#     mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "#     return mean\n",
    "\n",
    "# def  word_averaging_list(model, text_list):\n",
    "#     return np.vstack([word_averaging(model, post) for post in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_reviews = []\n",
    "# for text in X_train:\n",
    "#     train_reviews.append(getwordlist(text))\n",
    "# test_reviews=[]\n",
    "# for text in X_test:\n",
    "#     test_reviews.append(getwordlist(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique words 46908\n",
      "Train data shape: (1600, 2365)\n",
      "Test data shape: (400, 2365)\n"
     ]
    }
   ],
   "source": [
    "#GRU\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_reviews + test_reviews) \n",
    "sequences = tokenizer.texts_to_sequences(train_reviews)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('unique words %s' % len(word_index))\n",
    "\n",
    "train_data = pad_sequences(sequences, maxlen=2365, padding='post')\n",
    "print('Train data shape:', train_data.shape)\n",
    "\n",
    "test_data = pad_sequences(test_sequences, maxlen=2365, padding='post')\n",
    "print('Test data shape:', test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "embedding_matrix = np.zeros((len(word_index)+1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in model.vocab:\n",
    "        embedding_matrix[i] = model.word_vec(word)\n",
    "import keras\n",
    "embedding_layer = keras.layers.Embedding(len(word_index)+1, EMBEDDING_DIM, weights=[embedding_matrix],input_length=2365,trainable=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 2365, 300)         14072700  \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 2365, 300)         180300    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 300)               180300    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 14,433,601\n",
      "Trainable params: 360,901\n",
      "Non-trainable params: 14,072,700\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "1600/1600 [==============================] - 338s 211ms/step - loss: 0.1307 - acc: 0.0869 5:09 - loss: 0.7329 - acc: 0.271 - ETA: 3:34 - loss: 0.5815 - acc: 0\n",
      "Epoch 2/5\n",
      "1600/1600 [==============================] - 270s 169ms/step - loss: -0.1249 - acc: 0.0063\n",
      "Epoch 3/5\n",
      "1600/1600 [==============================] - 276s 173ms/step - loss: 0.0247 - acc: 0.0163\n",
      "Epoch 4/5\n",
      "1600/1600 [==============================] - 286s 179ms/step - loss: 0.0278 - acc: 0.0156\n",
      "Epoch 5/5\n",
      "1600/1600 [==============================] - 303s 189ms/step - loss: -0.0434 - acc: 0.0125\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import SimpleRNN, GRU \n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Flatten\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "#model.add(GRU(32))\n",
    "model.add(SimpleRNN(300, return_sequences=True))\n",
    "model.add(SimpleRNN(300))\n",
    "#model.add(Flatten())\n",
    "#model.add(SimpleRNN(32, return_sequences=True))\n",
    "#model.add(SimpleRNN(64))\n",
    "#model.add(SimpleRNN(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "print(model.summary())\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(train_data, y_train, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 40s 100ms/step\n",
      "Accuracy: 1.250000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, GlobalMaxPooling1D, MaxPool1D, Conv1D, Concatenate\n",
    "from keras.layers import Input\n",
    "text_seq_input = Input(shape=(2365,), dtype='int32')\n",
    "text_embedding = keras.layers.Embedding(len(word_index)+1, EMBEDDING_DIM, weights=[embedding_matrix],input_length=2365,trainable=False)(text_seq_input)\n",
    "\n",
    "filter_sizes = [3,4,5]\n",
    "convs = []\n",
    "for filter_size in filter_sizes:\n",
    "    l_conv = Conv1D(filters=128, kernel_size=filter_size, padding='same', activation='relu')(text_embedding)\n",
    "    l_pool = MaxPool1D(filter_size)(l_conv)\n",
    "    convs.append(l_pool)\n",
    "\n",
    "l_merge = Concatenate(axis=1)(convs)\n",
    "l_cov1= Conv1D(128, 5, activation='tanh')(l_merge)\n",
    "# since the text is too long we are maxpooling over 100\n",
    "# and not GlobalMaxPool1D\n",
    "l_pool1 = MaxPool1D(100)(l_cov1)\n",
    "l_flat = Flatten()(l_pool1)\n",
    "l_dense = Dense(128, activation='tanh')(l_flat)\n",
    "l_out = Dense(1, activation='softmax')(l_dense)\n",
    "model_1 = Model(inputs=[text_seq_input], outputs=l_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 2365)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_3 (Embedding)         (None, 2365, 300)    14072700    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 2365, 128)    115328      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 2365, 128)    153728      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 2365, 128)    192128      embedding_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 788, 128)     0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 591, 128)     0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 473, 128)     0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1852, 128)    0           max_pooling1d_5[0][0]            \n",
      "                                                                 max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 1848, 128)    82048       concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 18, 128)      0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2304)         0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 128)          295040      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            129         dense_4[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 14,911,101\n",
      "Trainable params: 838,401\n",
      "Non-trainable params: 14,072,700\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/5\n",
      "1600/1600 [==============================] - 852s 532ms/step - loss: 16.2014 - acc: 0.49195:43 - loss: 15.8801 - acc: 0. - ETA: 3:43 - loss: 16.3472 - acc: 0.48 - ETA: 2:50 - loss: 16.2911 - acc: 0.48 - ETA: 34s - loss: 16.2745 - acc: 0.4896\n",
      "Epoch 2/5\n",
      "1600/1600 [==============================] - 915s 572ms/step - loss: 16.2014 - acc: 0.49195:01 - loss: 15.8490 - acc: 0.50 - ETA: 3:01 - loss: 15.9922 - acc: 0.49 - ETA: 36s - loss: 16.0462 - acc: 0.4967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\keras\\callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (13.166872). Check your callbacks.\n",
      "  % delta_t_median)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5\n",
      "1600/1600 [==============================] - 468s 293ms/step - loss: 16.2014 - acc: 0.49193:27 - loss: 15.8801 - acc: 0.50 - ETA: 2:53 - loss: 16.1085 - acc: 0.49 - ETA: 2:05 - loss: 15.6310 - acc: 0. - ETA: 17s - loss: 16.0047 - acc: 0.4980\n",
      "Epoch 4/5\n",
      "1600/1600 [==============================] - 459s 287ms/step - loss: 16.2014 - acc: 0.49195:26 - loss: 15.3196 - acc: 0. - ETA: 2:28 - loss: 16.0981 - acc: 0.49 - ETA: 1:26 - loss: 16.2164 - acc: 0.4\n",
      "Epoch 5/5\n",
      "1600/1600 [==============================] - 416s 260ms/step - loss: 16.2014 - acc: 0.4919\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d8bbc59940>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model_1.summary()\n",
    "model_1.fit(train_data, y_train, epochs=5, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400/400 [==============================] - 39s 97ms/step\n",
      "Accuracy: 53.250000\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_1.evaluate(test_data, y_test, verbose=1)\n",
    "print('Accuracy: %f' % (accuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
