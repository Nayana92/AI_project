{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\smart_open\\ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import os\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "#turn doc into clean docs\n",
    "def doc_to_clean_lines(doc):\n",
    "    clean_lines = list()\n",
    "    lines = doc.splitlines()\n",
    "    for line in lines:\n",
    "        # split into tokens by white space\n",
    "        tokens = line.split()\n",
    "        # remove punctuation from each token\n",
    "        table = str.maketrans('', '', punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        # filter out tokens not in vocab\n",
    "        tokens = ' '.join(tokens)\n",
    "        clean_lines.append(tokens)\n",
    "    return clean_lines\n",
    "\n",
    "# load all docs in a directory\n",
    "def process_docs(directory, is_trian):\n",
    "    lines = list()\n",
    "    # walk through all files in the folder\n",
    "    for filename in os.listdir(directory):\n",
    "        # create the full path of the file to open\n",
    "        path = directory + '/' + filename\n",
    "        # load the doc\n",
    "        doc = load_doc(path)\n",
    "        doc_lines = doc_to_clean_lines(doc)\n",
    "        # add to list\n",
    "        lines +=doc_lines\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64720\n"
     ]
    }
   ],
   "source": [
    "# add all docs to vocab\n",
    "negative_docs = process_docs('txt_sentoken/neg', True)\n",
    "positive_docs = process_docs('txt_sentoken/pos', True)\n",
    "# df = \n",
    "train_docs = negative_docs + positive_docs\n",
    "print(len(train_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                comments  label\n",
      "7080   one of them might not be the person they are p...     -1\n",
      "21940  but goldblum just stands there for about thirt...     -1\n",
      "8637                              dont waste your money      -1\n",
      "11200  there is  in fact  no doubt about who this mot...     -1\n",
      "22809  its a startlingbuteffective visual contrast th...      1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "data = negative_docs\n",
    "neg_data = pd.DataFrame()\n",
    "neg_data['comments'] = negative_docs\n",
    "neg_data['label'] = -1\n",
    "pos_data = pd.DataFrame()\n",
    "pos_data['comments'] = positive_docs\n",
    "pos_data['label'] = 1\n",
    "#merge data\n",
    "data = pd.concat([neg_data,pos_data], axis=0)\n",
    "#shuffle the data\n",
    "data = shuffle(data)\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.vectors_norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "#         logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(model, text_list):\n",
    "    return np.vstack([word_averaging(model, post) for post in text_list ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data.comments\n",
    "y = data.label\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens\n",
    "    \n",
    "# train, test = train_test_split(df, test_size=0.3, random_state = 42)\n",
    "\n",
    "test_tokenized = X_test.apply(w2v_tokenize_text)#X_test.apply(lambda r: w2v_tokenize_text(r['comments'])).values\n",
    "train_tokenized = X_train.apply(w2v_tokenize_text)#X_train.apply(lambda r: w2v_tokenize_text(r['comments'])).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(model,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC \n",
    "import time\n",
    "\n",
    "# Guassian kernel\n",
    "start_time = time.time()\n",
    "svclassifier = SVC(kernel='rbf')  \n",
    "svclassifier.fit(X_train_word_average, y_train) \n",
    "\n",
    "y_pred = svclassifier.predict(X_test_word_average)  \n",
    "end_time = time.time()\n",
    "print(\"time taken \", end_time-start_time)\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg = logreg.fit(X_train_word_average, train['comments'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % accuracy_score(y_pred, test.tags))\n",
    "print(classification_report(test.tags, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.6122270292542233\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    positive       0.62      0.52      0.57      9467\n",
      "    negative       0.61      0.70      0.65      9949\n",
      "\n",
      "   micro avg       0.61      0.61      0.61     19416\n",
      "   macro avg       0.61      0.61      0.61     19416\n",
      "weighted avg       0.61      0.61      0.61     19416\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sgd = Pipeline([\n",
    "#                 ('vect', CountVectorizer()),\n",
    "#                 ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n",
    "               ])\n",
    "sgd.fit(X_train_word_average, y_train)\n",
    "\n",
    "# %%time\n",
    "\n",
    "y_pred = sgd.predict(X_test_word_average)\n",
    "my_tags = ['positive','negative']\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nayan\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SVC(kernel=linear)\n",
      "Training time: 8.235982s; Prediction time: 0.919539s\n",
      "positive:  {'precision': 0.7787610619469026, 'recall': 0.88, 'f1-score': 0.8262910798122066, 'support': 100}\n",
      "negative:  {'precision': 0.8620689655172413, 'recall': 0.75, 'f1-score': 0.8021390374331552, 'support': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# train Data\n",
    "trainData = pd.read_csv(\"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/train.csv\")\n",
    "\n",
    "# test Data\n",
    "testData = pd.read_csv(\"https://raw.githubusercontent.com/Vasistareddy/sentiment_analysis/master/data/test.csv\")\n",
    "\n",
    "# Create feature vectors\n",
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(trainData['Content'])\n",
    "test_vectors = vectorizer.transform(testData['Content'])\n",
    "\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='rbf')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, trainData['Label'])\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "# results\n",
    "print(\"Results for SVC(kernel=linear)\")\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "report = classification_report(testData['Label'], prediction_linear, output_dict=True)\n",
    "print('positive: ', report['pos'])\n",
    "print('negative: ', report['neg'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
